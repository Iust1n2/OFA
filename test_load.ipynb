{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import OFATokenizer, OFAModel\n",
    "from transformers.models.ofa.generate import sequence_generator\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/iustingrigoras/Desktop/OFA/OFA-base\n",
      "<super: <class 'OFATokenizer'>, <OFATokenizer object>>\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = \"/Users/iustingrigoras/Desktop/OFA/OFA-base\"\n",
    "tokenizer = OFATokenizer.from_pretrained(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OFAModel.from_pretrained(ckpt_dir, use_cache=True, output_atentions=True, output_hidden_states=True)\n",
    "generator = sequence_generator.SequenceGenerator(\n",
    "                    tokenizer=tokenizer,\n",
    "                    beam_size=3,\n",
    "                    max_len_b=10, \n",
    "                    min_len=0,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    temperature=0.5,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use Metal\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_file = \"/Users/iustingrigoras/Desktop/OFA/OFA-base/pytorch_model.bin\"\n",
    "ckpt = torch.load(ckpt_file)\n",
    "model = OFAModel.from_pretrained(ckpt_dir, use_cache=True, output_atentions=True, output_hidden_states=True)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_transform():\n",
    "    # Normalize using the mean and standard deviation that were used for model training\n",
    "    # These values are typically for ImageNet if not specified otherwise by the model\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),            # Convert numpy array or tensor to PIL Image\n",
    "        transforms.Resize((224, 224)),      # Resize to the input size that the model expects\n",
    "        transforms.ToTensor(),              # Convert PIL Image to a tensor\n",
    "        normalize,                          # Normalize the image\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image(self, image_location: str):\n",
    "        '''\n",
    "        This helper function takes the path to an image (either an URL or a local path) and\n",
    "        returns the image as an numpy array.\n",
    "        '''\n",
    "        if image_location.startswith('http'):\n",
    "            urllib.request.urlretrieve(image_location, 'temp.jpg')\n",
    "            image_location = 'temp.jpg'\n",
    "\n",
    "        img = Image.open(image_location).convert('RGB')\n",
    "        img = np.array(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(self, input_text, image, answer=None, person_info=None):\n",
    "        if not input_text:\n",
    "            input_text = ''\n",
    "        if answer is None:\n",
    "            input_ids = self.tokenizer.encode(input_text, return_tensors='pt', padding=True, truncation=True)\n",
    "            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)  # all*input_ids\n",
    "            txt_type_ids = torch.zeros_like(input_ids)\n",
    "        else:\n",
    "            input_ids_q = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(input_text))\n",
    "            input_ids_c = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(answer))\n",
    "            input_ids = [torch.tensor(self.tokenizer.build_inputs_with_special_tokens(input_ids_q, input_ids_c))]\n",
    "            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)  # all*input_ids\n",
    "\n",
    "            txt_type_ids = torch.tensor((len(input_ids_q) + 2 )* [0] + (len(input_ids_c) + 1) * [2])\n",
    "\n",
    "        position_ids = torch.arange(0, input_ids.size(1), dtype=torch.long).unsqueeze(0)\n",
    "        num_sents = [input_ids.size(0)]       \n",
    "        txt_lens = [i.size(0) for i in input_ids]\n",
    "\n",
    "        if image is None:\n",
    "            images_batch = None\n",
    "        else:\n",
    "            images_batch = torch.as_tensor(image.copy(), dtype=torch.float32)\n",
    "            images_batch = get_transform(images_batch.permute(2, 0, 1))\n",
    "\n",
    "        batch = {'input_ids': input_ids, 'txt_type_ids': txt_type_ids, 'position_ids': position_ids, 'images': images_batch,\n",
    "                \"txt_lens\": txt_lens, \"num_sents\": num_sents, 'person_info': person_info}\n",
    "        batch = self.move_to_device(batch)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "def data_setup(self, ex_id, image_location, input_text):\n",
    "    image = self.fetch_image(image_location) if image_location else None\n",
    "\n",
    "    batch = self.build_batch(input_text, image, answer=None, person_info=None)\n",
    "    scores, hidden_states, attentions = self.model(batch,\n",
    "                                                   compute_loss=False,\n",
    "                                                    output_attentions=True,\n",
    "                                                    output_hidden_states=True)\n",
    "\n",
    "    attentions = torch.stack(attentions).transpose(1,0).detach().cpu()[0]\n",
    "\n",
    "    if batch['images'] is None:\n",
    "        img, img_coords = np.array([]), []\n",
    "        len_img = 0\n",
    "    else:\n",
    "        image1, mask1 = self.model.preprocess_image(batch['images'].to(self.device))\n",
    "        image1 = (image1 * self.model.pixel_std + self.model.pixel_mean) * mask1\n",
    "        img = image1.cpu().numpy().astype(int).squeeze().transpose(1,2,0)\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        h0, w0 = h//64, w//64\n",
    "        len_img = w0 * h0\n",
    "        img_coords = np.fliplr(list(np.ndindex(h0, w0)))\n",
    "\n",
    "    input_ids = batch['input_ids'].cpu()\n",
    "    len_text = input_ids.size(1)\n",
    "    txt_tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0, :len_text])\n",
    "\n",
    "    len_tokens = len_text + len_img\n",
    "    attentions = attentions[:, :, :len_tokens, :len_tokens]\n",
    "    hidden_states = [hs[0].detach().cpu().numpy()[:len_tokens] for hs in hidden_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data_setup() missing 1 required positional argument: 'input_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples/family.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the family doing?\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m [data_setup(i, img, txt) \u001b[38;5;28;01mfor\u001b[39;00m i, (img, txt) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(images, texts))]\n",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples/family.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the family doing?\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[43mdata_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, (img, txt) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(images, texts))]\n",
      "\u001b[0;31mTypeError\u001b[0m: data_setup() missing 1 required positional argument: 'input_text'"
     ]
    }
   ],
   "source": [
    "images = [\"examples/family.jpeg\"]\n",
    "texts = [\"what is the family doing?\"]\n",
    "data = [data_setup(i, img, txt) for i, (img, txt) in enumerate(zip(images, texts))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
